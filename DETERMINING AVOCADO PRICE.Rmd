---
title: "AoL DMV Kelompok 3"
author: "Data Mining & Visualization"
date: '2022-05-24'
output: html_document
---

Group 3
Matthew Aaron Sugiyarto - 2501983742
Tifara Beata Wibowo - 2501975223
Reyner Wongso - 2501959586
Lindawaty Veronica - 2501972890
Ferry Irwanto - 2501987204

#Importing Library
```{r}
library("RCurl")
library("Hmisc")
library("caret")
```

#Importing Data
```{r}
urlfile <-'https://raw.githubusercontent.com/Lindaaa0/Dataset/main/avocado.csv'
data <- read.csv(urlfile, stringsAsFactors = FALSE)
data
```

#Changing Variable Name
```{r}
colnames(data)[5] <- "S.Avocado"
colnames(data)[6] <- "M.Avocado"
colnames(data)[7] <- "L.Avocado"
data
```

Variable Information :
1. Date - The date of the observation
2. AveragePrice - the average price of a single avocado
3. type - conventional or organic
4. year - the year
5. Region - the city or region of the observation
6. Total Volume - Total number of avocados sold
7. S. Avocado - small size avocado sold
8. M. Avocado - medium size avocado sold
9. L. Avocado - large size avocado sold

#Summarize
```{r}
dim(data)
```

There are 18.249 observations and 14 variables.

```{r}
sapply(data, class)
```

1. There are 2 variables with the data type of integer which are x and year.
2. There are 3 variables with the data type of character which are Date, type and region.
3. The rest of the variables have numeric data type.

```{r}
summary(data)
```

These are the numerical summary of each variables in this dataset.

```{r}
colSums(is.na(data))
```

There are no missing values in this dataset.

#Outliers
```{r}
FindOutliers <- function(data, t3 = 3, tH = 3, tb = 1.5){
 threeLims <- ThreeSigma(data, t = t3)
 HampLims <- Hampel(data)
 boxLims <- BoxplotRule(data, t = tb)

 n <- length(data)
 nMiss <- length(which(is.na(data)))

 threeList <- ExtractDetails(data, threeLims$down, threeLims$up)
 HampList <- ExtractDetails(data, HampLims$down, HampLims$up)
 boxList <- ExtractDetails(data, boxLims$down, boxLims$up)

 sumFrame <- data.frame(method = "ThreeSigma", n = n,
 nMiss = nMiss, nOut = threeList$nOut,
 lowLim = threeList$lowLim,
 upLim = threeList$upLim,
 minNom = threeList$minNom,
 maxNom = threeList$maxNom)
 upFrame <- data.frame(method = "Hampel", n = n,
 nMiss = nMiss, nOut = HampList$nOut,
 lowLim = HampList$lowLim,
 upLim = HampList$upLim,
 minNom = HampList$minNom,
 maxNom = HampList$maxNom)
 sumFrame <- rbind.data.frame(sumFrame, upFrame)
 upFrame <- data.frame(method = "BoxplotRule", n = n,
 nMiss = nMiss, nOut = boxList$nOut,
 lowLim = boxList$lowLim,
 upLim = boxList$upLim,
 minNom = boxList$minNom,
 maxNom = boxList$maxNom)
 sumFrame <- rbind.data.frame(sumFrame, upFrame)

 threeFrame <- data.frame(index = threeList$index,
 values = threeList$values,
 type = threeList$outClass)
 HampFrame <- data.frame(index = HampList$index,
 values = HampList$values,
 type = HampList$outClass)
 boxFrame <- data.frame(index = boxList$index,
 values = boxList$values,
 type = boxList$outClass)
 outList <- list(summary = sumFrame)
 return(outList)
}

FindOutliers(data$AveragePrice)
FindOutliers(data$Total.Volume)
FindOutliers(data$Total.Bags)
```

We decided to use "AveragePrice", "Total.Volume", "Total.Bags" variables because those variables can be used for EDA (Exploratory Data Analysis). 
1. Average Price
  using 3 sigma method & hampel identifier we identified there are 131 outliers.
  using boxplotRule we identified there are 1.103 outliers.
  
2. Total Volume
  using 3 sigma method we identified there are 171 outliers.
  using hampel identifier we identified there are 3.828 outliers.
  using BoxplotRUle we identified there are 2.297 outliers.

3. Total Bags
  using 3 sigma method we identified there are 287 outliers.
  using hampel identifier we identified there are 2.946 outliers.
  using boxplotRule we identified there are 2.533 outliers.

```{r}
no_outliers <- subset(data, data$Total.Volume > -200223.3 & data$Total.Volume < 1066147.9)
dim(no_outliers)
data <- no_outliers
data
```

We decided to use BoxplotRule and Total.Volume variable to remove unnecesarry outliers in this data because BoxplotRule is not too tolerant or too tolerant in its detection, unlike 3 sigma method and hampel identifier. Total.Volume is used because it will later be used as the dependent variable in this EDA.
By eliminating all outlier using BoxplotRule on Total.Volume we identify there are 15.952 observations that are counted as a non outliers.

```{r}
col <- c("Date", "type", "region")
data = data[,!(names(data) %in% col)]
data
dim(data)
```

Here we exclude unimportant variables such as "Date", "type", and "region" variables. The number of observations remain, but there are only 11 variables left.

#Exploring the Data
```{r}
rcorr(as.matrix(data), type = "spearman")
```

From the rcorr function we conclude that a lot of variables correlates strongly with each other.
One such variable is Total.Volume which has a strong correlation with 8 other variables. This makes Total.Volume a prime candidate as a dependent variable.

However, the variables which correlate highly with Total.Volume also correlate with each other. Hence, we decided to not use the whole 8 variables, and delete a few of them which correlate too highly with one another.

Because of these reasons, we decided to choose these variables as our independent & dependent variables.
independent variable : average price, total bags, L.Avocado
dependent variable : total volume

```{r}
hist(data)
```

Here we see that only 1 variable is close to be normally distributed. The rest of the variables does not have normal distribution.

```{r}
pairs(data)
```

Here we can see that Total.Volume variable has a convincing correlation with other variables. Other variables also have strong linear correlation with each other such as Total.Bags with Small.Bags and Total.Bags with Total.Volume.

```{r}
#drop column
drop <- c("X", "S.Avocado", "M.Avocado", "Small.Bags", "Large.Bags", "XLarge.Bags", "year")
  
data = data[,!(names(data) %in% drop)]
print('Modified dataframe:-')
data
```

In this process we drop a few of variables which are not fit to be independent variables because they correlate highly with each other. These variables are "X", "S.Avocado", "M.Avocado", "Small.Bags", "Large.Bags", "XLarge.Bags" and "year".

#Predictive Modelling
```{r}
##Divide Between Validation and Training sets

set.seed(42)

validationIndex = createDataPartition(data$Total.Volume, p=0.8, list = FALSE)
validation = data[-validationIndex,]
training = data[validationIndex,]

dim(validation)
dim(training)
```

This  step divides the dataset into two types via partition. The first partition with 3.188 observations is used to validate the predictive model, while the one with 12.764 is used to train the model.

```{r}
fit1 = lm(Total.Volume~., data = data)
summary(fit1)
plot(fit1, which = 1)

```

This step is used to test the first prototype of the predictive model, and we see fairly good results.

Based on the summary we can see that all the variables are statistically significant with all having three stars.

The Residuals vs Fitted of this prototype is satisfactory. A good Residuals vs Fitted model should have observations all over the place without concentrating on one point. Our model has even distribution along the x axis, but we can't say the same for the y axis. This means that the model may suffer from accuracy loss.

```{r}
prediction <- predict(fit1, validation)
mse <- mean((validation$Total.Volume - prediction)^2)
print(mse)
```

This step calculates the mean squared error of the model. The mse value of the current model is 14006399202.

```{r}
sigma(fit1)/mean(validation$Total.Volume)
```

This step calculates the coefficient of variation. The coefficient of variation of the current model is 0.6650404.

```{r}
validation$predictedVolume <- predict(fit1, validation)

truePredictedVolume <- data.frame(validation$Total.Volume, validation$predictedVolume,
                                 validation$Total.Volume - validation$predictedVolume)

names(truePredictedVolume) <- c("Volume", "Predicted", "residuals")

correlationAccuracy <- cor(truePredictedVolume)
correlationAccuracy
```

This step calculates the accuracy of the current model. The result is that the current model has an accuracy of around 86%, a satisfactory value because it is not too low (<60%) or too high (>90%).

#Predictive Modelling Plot
```{r}
##Finalizing Plot
predictioned <- predict(fit1, validation)

plot(predictioned, validation$Total.Volume, xlab = "Total Volume", ylab = "Actual Total Volume", col = "red", main = "Predicted vs Actual Total Volume of Avocado\n")
abline(a=0, b=1)
```

The graph shows the linear regression line of total avocado volume and the prediction produced by the model. The model shows that the red dots generally follow the straight line, with dispersing observations along the higher values. This means that the model can estimate the total volume quite well in lower values, but accuracy generally falls when predicting high amounts of avocado volume.

#Conclusion
Model
The model used to predict total avocado volume using average price, total bags and large avocados has an accuracy of 86%. This shows that the model has satisfying accuracy.
